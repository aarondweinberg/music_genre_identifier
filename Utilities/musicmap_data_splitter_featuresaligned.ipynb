{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c20c29",
   "metadata": {},
   "source": [
    "The goal of this notebook is to split the musicmap data (both audio files and features) into training, validation, and testing sets.\n",
    "\n",
    "We'll target a 70-20-10 split.\n",
    "\n",
    "Note that different songs have wildly different lengths. The shortest are under 1 minute, and others are over an hour.\n",
    "\n",
    "We'd like to have a relatively uniform distribution of clips among genres and, ideally, songs within genre. So we've decided to limit the number of clips to 4 (randomly selected) per song. A few genres will be a bit short.\n",
    "\n",
    "The code below will:\n",
    "1. Recursively find all audio files in subdirectories (by genre).\n",
    "2. Randomly split each genre into 70% train, 20% validation, 10% test.\n",
    "3. Create a CSV listing the file path and its assigned split.\n",
    "4. Copy files into new train/, validation/, and test/ folders while preserving the subdirectory (genre) structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad4b4c",
   "metadata": {},
   "source": [
    "First we'll generate the splits and create the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb75102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_splits.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03846d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "source_dir = Path('Musicmap_Improved_Download')\n",
    "output_base = Path('Musicmap_Dataset_Split')                 # Will contain train/val/test folders\n",
    "segment_length = 30\n",
    "audio_extensions = {'.mp3', '.wav', '.flac', '.m4a'}\n",
    "split_ratios = {'train': 0.7, 'val': 0.2, 'test': 0.1}\n",
    "max_segments_per_song = 4\n",
    "\n",
    "# libaudio is deprecated in librosa, and ffmpeg is slow because it loads the entire audio file into memory\n",
    "# using ffprobe instead\n",
    "def get_duration_ffprobe(path):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ffprobe', '-v', 'error', '-show_entries',\n",
    "             'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', str(path)],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "        return float(result.stdout.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"ffprobe failed on {path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "#def get_duration_ffmpeg(path, sr=22050):\n",
    "#    try:\n",
    "#        command = [\n",
    "#            'ffmpeg', '-i', str(path),\n",
    "#            '-f', 'f32le', '-acodec', 'pcm_f32le',\n",
    "#            '-ac', '1', '-ar', str(sr), '-loglevel', 'quiet', '-'\n",
    "#        ]\n",
    "#        out = subprocess.check_output(command)\n",
    "#        y = np.frombuffer(out, dtype=np.float32)\n",
    "#        return librosa.get_duration(y=y, sr=sr)\n",
    "#    except Exception as e:\n",
    "#        print(f\"ffmpeg decode failed on {path}: {e}\")\n",
    "#        return 0\n",
    "\n",
    "# === STEP 1: Gather files by genre ===\n",
    "genre_to_files = {}\n",
    "for genre_dir in source_dir.iterdir():\n",
    "    if genre_dir.is_dir():\n",
    "        files = [f for f in genre_dir.glob('*') if f.suffix.lower() in audio_extensions]\n",
    "        if files:\n",
    "            genre_to_files[genre_dir.name] = files\n",
    "\n",
    "# === STEP 2: Shuffle and split ===\n",
    "file_records = []\n",
    "\n",
    "for genre, files in genre_to_files.items():\n",
    "    random.shuffle(files)\n",
    "    n = len(files)\n",
    "    n_train = int(n * split_ratios['train'])\n",
    "    n_val = int(n * split_ratios['val'])\n",
    "\n",
    "    split_data = [\n",
    "        ('train', files[:n_train]),\n",
    "        ('val', files[n_train:n_train + n_val]),\n",
    "        ('test', files[n_train + n_val:])\n",
    "    ]\n",
    "\n",
    "    for split_name, file_list in split_data:\n",
    "        for file_path in file_list:\n",
    "            duration = get_duration_ffprobe(file_path)\n",
    "            num_segments = int(duration // segment_length)\n",
    "            selected_segments = min(num_segments, max_segments_per_song)\n",
    "            relative_path = file_path.relative_to(source_dir)\n",
    "\n",
    "            file_records.append({\n",
    "                'split': split_name,\n",
    "                'genre': genre,\n",
    "                'file_path': str(relative_path),\n",
    "                'filename': file_path.stem,\n",
    "                'num_segments': num_segments,\n",
    "                'num_selected_segments': selected_segments\n",
    "            })\n",
    "\n",
    "# === STEP 3: Sort and write CSVs ===\n",
    "output_base.mkdir(exist_ok=True)\n",
    "file_records.sort(key=lambda r: (r['genre'], r['split']))\n",
    "\n",
    "fieldnames = ['split', 'genre', 'file_path', 'filename', 'num_segments', 'num_selected_segments']\n",
    "\n",
    "# Write full CSV\n",
    "with open(output_base / 'file_split.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(file_records)\n",
    "\n",
    "# Write split CSVs\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_records = [r for r in file_records if r['split'] == split]\n",
    "    with open(output_base / f'file_split_{split}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(split_records)\n",
    "\n",
    "print(f\"CSV files written to {output_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d68d4",
   "metadata": {},
   "source": [
    "If desired, we can copy the audio files. But all we really need are the features, so this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841be0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_files_from_csv.py\n",
    "\n",
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "source_dir = Path('Musicmap_Improved_Download')\n",
    "csv_path = Path('Musicmap_Dataset_Split/file_split.csv')  # Or file_split_train.csv etc.\n",
    "destination_base = Path('Musicmap_Dataset_Split')         # Will contain train/, val/, test/\n",
    "\n",
    "# === Read CSV ===\n",
    "with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    records = list(reader)\n",
    "\n",
    "# === Copy Files ===\n",
    "for record in records:\n",
    "    split_dir = destination_base / record['split'] / record['genre']\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    src = source_dir / record['file_path']\n",
    "    dst = split_dir / src.name\n",
    "\n",
    "    if not src.exists():\n",
    "        print(f\"Missing: {src}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        shutil.copy2(src, dst)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {src} → {dst}: {e}\")\n",
    "\n",
    "print(f\"\\nFiles copied based on {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239436f",
   "metadata": {},
   "source": [
    "Now we'll create the splits for the features. This will:\n",
    "\n",
    "1. Reads the file_split.csv (or the separate train/val/test CSVs) generated above\n",
    "2. For each feature directory (e.g., chroma_bs_gray, hpss_mean, etc.):\n",
    "    1. Iterate over genres and filenames.\n",
    "    2. Match all *_seg*.png segment files associated with each base filename.\n",
    "    3. Copy those segment files into the corresponding new directory:\n",
    "        * musicmap_processed_output_train/<feature>/<genre>/...\n",
    "        * musicmap_processed_output_val/<feature>/<genre>/...\n",
    "        * musicmap_processed_output_test/<feature>/<genre>/...\n",
    "3. For the features_csv directory, it\n",
    "    1. Loads each CSV file\n",
    "    2. Filters rows based on the segment names found in file_split.csv\n",
    "    3. Saves filtered CSVs into new folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7340df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found feature directories: ['chroma_cq_gray', 'features_csv', 'chroma_gray', 'hpss_mean', 'hpss_median', 'resnet_mel_rgb', 'mfcc_gray', 'chroma_bs_gray', 'mfcc_plot', 'mel_pcen_gray', 'mel_db_gray']\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/chroma_cq_gray/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/chroma_gray/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/hpss_mean/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/hpss_median/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/resnet_mel_rgb/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/mfcc_gray/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/chroma_bs_gray/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/mfcc_plot/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/mel_pcen_gray/rnr03\n",
      "[WARNING] No segments found for rnr03-10-Ian Campbell folk Group-The Fireman’s Song in 15_second_features/musicmap_processed_output/mel_db_gray/rnr03\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/chroma_cq_gray/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/chroma_gray/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/hpss_mean/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/hpss_median/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/resnet_mel_rgb/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/mfcc_gray/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/chroma_bs_gray/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/mfcc_plot/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/mel_pcen_gray/uti02\n",
      "[WARNING] No segments found for uti02-4-Bedřich Smetana-The Moldau  in 15_second_features/musicmap_processed_output/mel_db_gray/uti02\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tonnetz_3.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tonnetz_3.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tonnetz_3.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tonnetz_2.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tonnetz_2.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tonnetz_2.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tonnetz_0.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tonnetz_0.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tonnetz_0.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tonnetz_1.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tonnetz_1.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tonnetz_1.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tonnetz_5.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tonnetz_5.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tonnetz_5.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/spec_centroid.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/spec_centroid.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/spec_centroid.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tonnetz_4.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tonnetz_4.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tonnetz_4.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/spec_flatness.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/spec_flatness.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/spec_flatness.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/spec_rolloff.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/spec_rolloff.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/spec_rolloff.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/spec_contrast.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/spec_contrast.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/spec_contrast.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/tempo.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/tempo.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/tempo.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/rms.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/rms.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/rms.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/zcr.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/zcr.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/zcr.csv\n",
      "[WARNING] Skipping CSV without 'segment' column: 15_second_features/musicmap_processed_output/features_csv/song_durations.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/features_normalized.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/features_normalized.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/features_normalized.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_train/features_csv/spec_bandwidth.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_val/features_csv/spec_bandwidth.csv\n",
      "✓ Wrote filtered CSV to 15_second_features/musicmap_processed_output_splits_test/features_csv/spec_bandwidth.csv\n",
      "Segment and feature CSV copying completed.\n"
     ]
    }
   ],
   "source": [
    "# copy_processed_segments.py\n",
    "\n",
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "csv_path = Path(\"Musicmap_Dataset_Split/file_split_15sec.csv\")\n",
    "processed_input_root = Path(\"15_second_features/musicmap_processed_output\")\n",
    "output_root_base = Path(\"15_second_features/musicmap_processed_output_splits\")  # Will create _train, _val, _test subdirs\n",
    "\n",
    "# === Read CSV and group by split ===\n",
    "with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    records = list(reader)\n",
    "\n",
    "# Map: split → set of full segment names (e.g., \"tec05-2-Artist-Title_seg0\")\n",
    "split_to_segments = {'train': set(), 'val': set(), 'test': set()}\n",
    "for r in records:\n",
    "    base = r['filename']\n",
    "    split = r['split']\n",
    "    num_segs = int(r.get(\"num_selected_segments\", r.get(\"num_segments\", 0)))\n",
    "    for i in range(num_segs):\n",
    "        seg_name = f\"{base}_seg{i}\"\n",
    "        split_to_segments[split].add(seg_name)\n",
    "\n",
    "# === Collect feature directories ===\n",
    "feature_dirs = [d for d in processed_input_root.iterdir() if d.is_dir()]\n",
    "print(f\"Found feature directories: {[d.name for d in feature_dirs]}\")\n",
    "\n",
    "# === Copy PNGs from image-based features ===\n",
    "for record in records:\n",
    "    split = record[\"split\"]\n",
    "    genre = record[\"genre\"]\n",
    "    base_filename = record[\"filename\"]\n",
    "\n",
    "    for feature_dir in feature_dirs:\n",
    "        if feature_dir.name == \"features_csv\":\n",
    "            continue  # skip until next step\n",
    "\n",
    "        source_genre_dir = feature_dir / genre\n",
    "        if not source_genre_dir.exists():\n",
    "            print(f\"[WARNING] Missing genre directory: {source_genre_dir}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        all_pngs = list(source_genre_dir.glob(\"*.png\"))\n",
    "        matched_segments = [f for f in all_pngs if f.name.startswith(f\"{base_filename}_seg\")]\n",
    "\n",
    "        if not matched_segments:\n",
    "            print(f\"[WARNING] No segments found for {base_filename} in {source_genre_dir}\")\n",
    "            continue\n",
    "\n",
    "        num_segs = int(record.get(\"num_selected_segments\", record.get(\"num_segments\", 0)))\n",
    "        if len(matched_segments) > num_segs:\n",
    "            matched_segments = random.sample(matched_segments, num_segs)\n",
    "\n",
    "# Code was previously not selecting random segments... fix is above\n",
    "#        all_pngs = list(source_genre_dir.glob(\"*.png\"))\n",
    "#        matched_segments = [f for f in all_pngs if f.name.startswith(f\"{base_filename}_seg\")]\n",
    "\n",
    "#        segment_pattern = str(source_genre_dir / f\"{base_filename}_seg*.png\")\n",
    "#        matched_segments = glob.glob(segment_pattern)\n",
    "\n",
    "        if not matched_segments:\n",
    "            print(f\"[WARNING] No segments found for {base_filename} in {source_genre_dir}\")\n",
    "            continue\n",
    "\n",
    "        dest_genre_dir = output_root_base.with_name(output_root_base.name + f\"_{split}\") / feature_dir.name / genre\n",
    "        dest_genre_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for seg_file in matched_segments:\n",
    "            seg_file_path = Path(seg_file)\n",
    "            dest_path = dest_genre_dir / seg_file_path.name\n",
    "            try:\n",
    "                shutil.copy2(seg_file_path, dest_path)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to copy {seg_file_path} → {dest_path}: {e}\")\n",
    "\n",
    "# === Handle features_csv filtering ===\n",
    "csv_feature_dir = processed_input_root / \"features_csv\"\n",
    "if csv_feature_dir.exists():\n",
    "    for csv_file in csv_feature_dir.glob(\"*.csv\"):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if \"segment\" not in df.columns:\n",
    "            print(f\"[WARNING] Skipping CSV without 'segment' column: {csv_file}\")\n",
    "            continue\n",
    "\n",
    "        for split, segments in split_to_segments.items():\n",
    "            filtered_df = df[df[\"segment\"].isin(segments)].copy()\n",
    "            if not filtered_df.empty:\n",
    "                out_dir = output_root_base.with_name(output_root_base.name + f\"_{split}\") / \"features_csv\"\n",
    "                out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                out_path = out_dir / csv_file.name\n",
    "                filtered_df.to_csv(out_path, index=False)\n",
    "                print(f\"✓ Wrote filtered CSV to {out_path}\")\n",
    "\n",
    "print(\"Segment and feature CSV copying completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aedb7d",
   "metadata": {},
   "source": [
    "<h3>Enhanced version with dry-run</h3>\n",
    "\n",
    "The version below doesn't work quite right - it randomly selects a different collection of files for each feature folder, rather than ensuring consistency in files across the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc43f932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found feature directories: ['chroma_cq_gray', 'features_csv', 'chroma_gray', 'hpss_mean', 'hpss_median', 'resnet_mel_rgb', 'mfcc_gray', 'chroma_bs_gray', 'mfcc_plot', 'mel_pcen_gray', 'mel_db_gray']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# === Entry Point ===\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or dry_run=False\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Enable below for command line useage\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m#if __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m#    import sys\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    dry_run = args.dry_run  # You can also hardcode here: dry_run = True\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m#    main(dry_run)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(dry_run)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     79\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERROR] Failed to copy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseg_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/deep-learning/lib/python3.12/shutil.py:475\u001b[39m, in \u001b[36mcopy2\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    473\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m copystat(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/deep-learning/lib/python3.12/shutil.py:260\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    258\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    262\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    263\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import shutil\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def main(dry_run=False):\n",
    "    # === CONFIGURATION ===\n",
    "    csv_path = Path(\"Musicmap_Dataset_Split/file_split_15sec.csv\")\n",
    "    processed_input_root = Path(\"15_second_features_augmented/musicmap_processed_output\")\n",
    "    output_root_base = Path(\"15_second_features_augmented/musicmap_processed_output_splits\")  # Will create _train, _val, _test subdirs\n",
    "\n",
    "    dry_run_log = []\n",
    "\n",
    "    # === Read CSV and group by split ===\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        records = list(reader)\n",
    "\n",
    "    # Map: split → set of full segment names (e.g., \"tec05-2-Artist-Title_seg0\")\n",
    "    split_to_segments = {'train': set(), 'val': set(), 'test': set()}\n",
    "    for r in records:\n",
    "        base = r['filename']\n",
    "        split = r['split']\n",
    "        num_segs = int(r.get(\"num_selected_segments\", r.get(\"num_segments\", 0)))\n",
    "        for i in range(num_segs):\n",
    "            seg_name = f\"{base}_seg{i}\"\n",
    "            split_to_segments[split].add(seg_name)\n",
    "\n",
    "    # === Collect feature directories ===\n",
    "    feature_dirs = [d for d in processed_input_root.iterdir() if d.is_dir()]\n",
    "    print(f\"Found feature directories: {[d.name for d in feature_dirs]}\")\n",
    "\n",
    "    # === Copy PNGs from image-based features ===\n",
    "    for record in records:\n",
    "        split = record[\"split\"]\n",
    "        genre = record[\"genre\"]\n",
    "        base_filename = record[\"filename\"]\n",
    "\n",
    "        for feature_dir in feature_dirs:\n",
    "            if feature_dir.name == \"features_csv\":\n",
    "                continue  # skip until next step\n",
    "\n",
    "            source_genre_dir = feature_dir / genre\n",
    "            if not source_genre_dir.exists():\n",
    "                msg = f\"[WARNING] Missing genre directory: {source_genre_dir}\"\n",
    "                print(msg)\n",
    "                dry_run_log.append(msg)\n",
    "                continue\n",
    "\n",
    "            all_pngs = list(source_genre_dir.glob(\"*.png\"))\n",
    "            matched_segments = [f for f in all_pngs if f.name.startswith(f\"{base_filename}_seg\")]\n",
    "\n",
    "            if not matched_segments:\n",
    "                msg = f\"[WARNING] No segments found for {base_filename} in {source_genre_dir}\"\n",
    "                print(msg)\n",
    "                dry_run_log.append(msg)\n",
    "                continue\n",
    "\n",
    "            num_segs = int(record.get(\"num_selected_segments\", record.get(\"num_segments\", 0)))\n",
    "            if len(matched_segments) > num_segs:\n",
    "                matched_segments = random.sample(matched_segments, num_segs)\n",
    "\n",
    "            dest_genre_dir = output_root_base.with_name(output_root_base.name + f\"_{split}\") / feature_dir.name / genre\n",
    "            if not dry_run:\n",
    "                dest_genre_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for seg_file in matched_segments:\n",
    "                seg_file_path = Path(seg_file)\n",
    "                dest_path = dest_genre_dir / seg_file_path.name\n",
    "                if dry_run:\n",
    "                    dry_run_log.append(f\"[DRY-RUN] Would copy {seg_file_path} → {dest_path}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        shutil.copy2(seg_file_path, dest_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Failed to copy {seg_file_path} → {dest_path}: {e}\")\n",
    "\n",
    "    # === Handle features_csv filtering ===\n",
    "    csv_feature_dir = processed_input_root / \"features_csv\"\n",
    "    if csv_feature_dir.exists():\n",
    "        for csv_file in csv_feature_dir.glob(\"*.csv\"):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if \"segment\" not in df.columns:\n",
    "                msg = f\"[WARNING] Skipping CSV without 'segment' column: {csv_file}\"\n",
    "                print(msg)\n",
    "                dry_run_log.append(msg)\n",
    "                continue\n",
    "\n",
    "            for split, segments in split_to_segments.items():\n",
    "                filtered_df = df[df[\"segment\"].isin(segments)].copy()\n",
    "                if not filtered_df.empty:\n",
    "                    out_dir = output_root_base.with_name(output_root_base.name + f\"_{split}\") / \"features_csv\"\n",
    "                    out_path = out_dir / csv_file.name\n",
    "                    if dry_run:\n",
    "                        dry_run_log.append(f\"[DRY-RUN] Would write filtered CSV to {out_path}\")\n",
    "                    else:\n",
    "                        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        filtered_df.to_csv(out_path, index=False)\n",
    "                        print(f\"✓ Wrote filtered CSV to {out_path}\")\n",
    "\n",
    "    # === Dry-run summary ===\n",
    "    if dry_run:\n",
    "        log_path = Path(\"dry_run_log.txt\")\n",
    "        with log_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for line in dry_run_log:\n",
    "                f.write(line + \"\\n\")\n",
    "        print(f\"\\n[DRY-RUN] Completed. Log written to {log_path}\")\n",
    "    else:\n",
    "        print(\"Segment and feature CSV copying completed.\")\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "if __name__ == \"__main__\":\n",
    "    main(dry_run=False)  # or dry_run=False\n",
    "\n",
    "# Enable below for command line useage\n",
    "#if __name__ == \"__main__\":\n",
    "#    import sys\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument('--dry-run', action='store_true', help='Only simulate copying, don’t copy files.')\n",
    "#\n",
    "#    # Avoid errors in Jupyter or VS Code notebooks by ignoring unknown args\n",
    "#    args, unknown = parser.parse_known_args()\n",
    "#\n",
    "#    dry_run = args.dry_run  # You can also hardcode here: dry_run = True\n",
    "#    main(dry_run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5842844",
   "metadata": {},
   "source": [
    "<h2>Enhanced, Corrected Version</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42084ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found feature directories: ['chroma_cq_gray', 'features_csv', 'chroma_gray', 'hpss_mean', 'hpss_median', 'resnet_mel_rgb', 'mfcc_gray', 'chroma_bs_gray', 'mfcc_plot', 'mel_pcen_gray', 'mel_db_gray']\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tonnetz_3.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tonnetz_3.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tonnetz_3.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tonnetz_2.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tonnetz_2.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tonnetz_2.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tonnetz_0.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tonnetz_0.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tonnetz_0.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tonnetz_1.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tonnetz_1.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tonnetz_1.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tonnetz_5.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tonnetz_5.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tonnetz_5.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/spec_centroid.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/spec_centroid.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/spec_centroid.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tonnetz_4.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tonnetz_4.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tonnetz_4.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/spec_flatness.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/spec_flatness.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/spec_flatness.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/spec_rolloff.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/spec_rolloff.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/spec_rolloff.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/spec_contrast.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/spec_contrast.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/spec_contrast.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/tempo.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/tempo.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/tempo.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/rms.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/rms.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/rms.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/zcr.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/zcr.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/zcr.csv\n",
      "[WARNING] Skipping CSV without 'segment' column: 15_second_features_augmented/musicmap_processed_output/features_csv/song_durations.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/features_normalized.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/features_normalized.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/features_normalized.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_train/features_csv/spec_bandwidth.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_val/features_csv/spec_bandwidth.csv\n",
      "✓ Wrote filtered CSV to 15_second_features_augmented/musicmap_processed_output_splits_test/features_csv/spec_bandwidth.csv\n",
      "Segment and feature CSV copying completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "def normalize_filename(s):\n",
    "    # Normalize Unicode and strip whitespace\n",
    "    return unicodedata.normalize(\"NFKC\", s).strip()\n",
    "\n",
    "def main(dry_run=False):\n",
    "    # === CONFIGURATION ===\n",
    "    csv_path = Path(\"Musicmap_Dataset_Split/file_split_15sec_augmented.csv\")\n",
    "    processed_input_root = Path(\"15_second_features_augmented/musicmap_processed_output\")\n",
    "    output_root_base = Path(\"15_second_features_augmented/musicmap_processed_output_splits\")  # Will create _train, _val, _test subdirs\n",
    "\n",
    "    dry_run_log = []\n",
    "\n",
    "    # === Read CSV and group by split ===\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        records = list(reader)\n",
    "\n",
    "    # Build dict: (genre, filename) → split, num_segs\n",
    "    record_lookup = {}\n",
    "    for r in records:\n",
    "        key = (r[\"genre\"], r[\"filename\"])\n",
    "        record_lookup[key] = {\n",
    "            \"split\": r[\"split\"],\n",
    "            \"num_segs\": int(r.get(\"num_selected_segments\", r.get(\"num_segments\", 0)))\n",
    "        }\n",
    "\n",
    "    # === Collect feature directories ===\n",
    "    feature_dirs = [d for d in processed_input_root.iterdir() if d.is_dir()]\n",
    "    print(f\"Found feature directories: {[d.name for d in feature_dirs]}\")\n",
    "\n",
    "    # === Determine selected segment files per file ===\n",
    "    selected_files_per_record = {}  # key: (genre, base_filename) → List[Path]\n",
    "\n",
    "    for (genre, base_filename), info in record_lookup.items():\n",
    "        num_segs = info[\"num_segs\"]\n",
    "\n",
    "        # Use first non-csv feature dir as reference\n",
    "        reference_dir = next((d for d in feature_dirs if d.name != \"features_csv\"), None)\n",
    "        if not reference_dir:\n",
    "            print(\"[ERROR] No reference feature directory found.\")\n",
    "            return\n",
    "\n",
    "        source_dir = reference_dir / genre\n",
    "        if not source_dir.exists():\n",
    "            msg = f\"[WARNING] Missing genre folder in {reference_dir.name}: {genre}\"\n",
    "            print(msg)\n",
    "            dry_run_log.append(msg)\n",
    "            continue\n",
    "\n",
    "        # Normalize base filename to NFC for reliable matching\n",
    "        normalized_base = normalize_filename(base_filename)\n",
    "\n",
    "        # Get all candidate segment files in the genre folder\n",
    "        all_candidates = list(source_dir.glob(\"*_seg*.png\"))\n",
    "\n",
    "        # Normalize and match using stem comparison\n",
    "        matched_segments = [\n",
    "            f for f in all_candidates\n",
    "            if normalize_filename(base_filename) in normalize_filename(f.stem)\n",
    "        ]\n",
    "\n",
    "        if len(all_candidates) < num_segs:\n",
    "            msg = f\"[WARNING] Only {len(all_candidates)} segments found for {base_filename}, requested {num_segs}\"\n",
    "            print(msg)\n",
    "            dry_run_log.append(msg)\n",
    "\n",
    "        selected = random.sample(matched_segments, min(len(matched_segments), num_segs))\n",
    "        selected_files_per_record[(genre, base_filename)] = selected\n",
    "\n",
    "    # === Copy selected segments to each feature directory ===\n",
    "    for (genre, base_filename), selected_files in selected_files_per_record.items():\n",
    "        split = record_lookup[(genre, base_filename)][\"split\"]\n",
    "\n",
    "        for feature_dir in feature_dirs:\n",
    "            if feature_dir.name == \"features_csv\":\n",
    "                continue  # Skip CSVs for now\n",
    "\n",
    "            source_genre_dir = feature_dir / genre\n",
    "            dest_genre_dir = output_root_base.with_name(output_root_base.name + f\"_{split}\") / feature_dir.name / genre\n",
    "\n",
    "            if not dry_run:\n",
    "                dest_genre_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for seg_file in selected_files:\n",
    "                src_path = source_genre_dir / seg_file.name\n",
    "                dest_path = dest_genre_dir / seg_file.name\n",
    "                if not src_path.exists():\n",
    "                    msg = f\"[WARNING] Missing file: {src_path}\"\n",
    "                    print(msg)\n",
    "                    dry_run_log.append(msg)\n",
    "                    continue\n",
    "\n",
    "                if dry_run:\n",
    "                    dry_run_log.append(f\"[DRY-RUN] Would copy {src_path} → {dest_path}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        shutil.copy2(src_path, dest_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Failed to copy {src_path} → {dest_path}: {e}\")\n",
    "\n",
    "    # === Handle features_csv filtering ===\n",
    "    csv_feature_dir = processed_input_root / \"features_csv\"\n",
    "    if csv_feature_dir.exists():\n",
    "        for csv_file in csv_feature_dir.glob(\"*.csv\"):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if \"segment\" not in df.columns:\n",
    "                msg = f\"[WARNING] Skipping CSV without 'segment' column: {csv_file}\"\n",
    "                print(msg)\n",
    "                dry_run_log.append(msg)\n",
    "                continue\n",
    "\n",
    "            for split in ['train', 'val', 'test']:\n",
    "                segments = {\n",
    "                    seg.name.replace(\".png\", \"\")\n",
    "                    for (genre, fname), segs in selected_files_per_record.items()\n",
    "                    if record_lookup[(genre, fname)][\"split\"] == split\n",
    "                    for seg in segs\n",
    "                }\n",
    "                filtered_df = df[df[\"segment\"].isin(segments)].copy()\n",
    "                if not filtered_df.empty:\n",
    "                    out_dir = output_root_base.with_name(output_root_base.name + f\"_{split}\") / \"features_csv\"\n",
    "                    out_path = out_dir / csv_file.name\n",
    "                    if dry_run:\n",
    "                        dry_run_log.append(f\"[DRY-RUN] Would write filtered CSV to {out_path}\")\n",
    "                    else:\n",
    "                        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        filtered_df.to_csv(out_path, index=False)\n",
    "                        print(f\"✓ Wrote filtered CSV to {out_path}\")\n",
    "\n",
    "    # === Dry-run summary ===\n",
    "    if dry_run:\n",
    "        log_path = Path(\"dry_run_log.txt\")\n",
    "        with log_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for line in dry_run_log:\n",
    "                f.write(line + \"\\n\")\n",
    "        print(f\"\\n[DRY-RUN] Completed. Log written to {log_path}\")\n",
    "    else:\n",
    "        print(\"Segment and feature CSV copying completed.\")\n",
    "\n",
    "\n",
    "# === Entry Point ===\n",
    "if __name__ == \"__main__\":\n",
    "    main(dry_run=False)  # Change to False when ready to actually copy files\n",
    "\n",
    "\n",
    "# Enable below for command line useage\n",
    "#if __name__ == \"__main__\":\n",
    "#    import sys\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    parser.add_argument('--dry-run', action='store_true', help='Only simulate copying, don’t copy files.')\n",
    "#\n",
    "#    # Avoid errors in Jupyter or VS Code notebooks by ignoring unknown args\n",
    "#    args, unknown = parser.parse_known_args()\n",
    "#\n",
    "#    dry_run = args.dry_run  # You can also hardcode here: dry_run = True\n",
    "#    main(dry_run)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
